# =============================================================================
# GITHUB COPILOT MODELS CONFIGURATION
# =============================================================================
# Get your free token from: https://github.com/settings/tokens
# GitHub Models provides free access to GPT-4o and other models for experimentation
GITHUB_TOKEN=your_github_personal_access_token_here
GITHUB_MODEL_ID=gpt-4o-mini  # Options: gpt-4o, gpt-4o-mini, gpt-4-turbo

# =============================================================================
# EMBEDDING MODEL CONFIGURATION
# =============================================================================
# Choose embedding provider: 'github' (free but rate-limited), 'ollama' (local), or 'lmstudio' (Windows with GPU)
EMBEDDING_PROVIDER=github  # Options: github, ollama, lmstudio

# LM Studio Configuration (when EMBEDDING_PROVIDER=lmstudio)
# Download LM Studio: https://lmstudio.ai
# Load an embedding model (e.g., nomic-embed-text) and start the server
# For WSL2: Get Windows host IP with: cat /etc/resolv.conf | grep nameserver | awk '{print $2}'
LMSTUDIO_URL=http://host.docker.internal:1234/v1  # Windows: host.docker.internal, WSL2: use Windows IP
LMSTUDIO_MODEL=text-embedding-nomic-embed-text-v2  # Must match model loaded in LM Studio

# Ollama Configuration (when EMBEDDING_PROVIDER=ollama)
# Install Ollama: https://ollama.ai
# Pull model: ollama pull nomic-embed-text
# For WSL2: Start Ollama with OLLAMA_HOST=0.0.0.0:11434 to bind to all interfaces
OLLAMA_HOST=http://172.17.0.1:11434  # For WSL2/Linux: use Docker gateway IP (172.17.0.1)
OLLAMA_EMBEDDING_MODEL=nomic-embed-text  # Options: nomic-embed-text (768D), mxbai-embed-large (1024D)

EMBEDDING_DIMENSIONS=768  # Must match model: nomic-embed-text=768, mxbai-embed-large=1024, github=1536

# =============================================================================
# INFRASTRUCTURE CONFIGURATION (auto-configured by Docker)
# =============================================================================

# Database Configuration
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_DB=techflow
POSTGRES_USER=techflow_user
POSTGRES_PASSWORD=techflow_pass_change_in_production

# Redis Configuration
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_PASSWORD=

# Vector Database Configuration
QDRANT_HOST=qdrant
QDRANT_PORT=6333
QDRANT_API_KEY=

# =============================================================================
# AI AGENT FEATURE FLAGS
# =============================================================================
# Enable/disable AI capabilities to see the impact of each agent on efficiency
# Set to 'true' to enable AI, 'false' to use manual/rule-based fallback
# This allows you to experience the transformation incrementally and measure ROI

# Phase 1: Quick Wins - Automating Simple Tasks
ENABLE_AI_SUPPORT_BOT=false          # Lesson 1: Auto-answer support tickets (50% reduction)
ENABLE_AI_FAQ_RAG=false              # Lesson 2: RAG-powered knowledge search (90% accuracy)
ENABLE_AI_SMART_ROUTER=false         # Lesson 3: Intelligent ticket routing (80% improvement)

# Phase 2: Workflow Automation - Connecting Systems
ENABLE_AI_SALES_ASSISTANT=false      # Lesson 4: Automated prospect research (70% time saved)
ENABLE_AI_CODE_REVIEWER=false        # Lesson 5: AI code review suggestions (50% faster)
ENABLE_AI_ONBOARDING_COACH=false     # Lesson 6: Interactive developer onboarding (40% faster)

# Phase 3: Multi-Agent Systems - Complex Problem Solving
ENABLE_AI_INVOICE_PROCESSOR=false    # Lesson 7: End-to-end invoice automation (90% automation)
ENABLE_AI_RESEARCH_TEAM=false        # Lesson 8: Multi-agent research system (10x faster)
ENABLE_AI_HR_ASSISTANT=false         # Lesson 9: Employee self-service bot (80% automation)

# Phase 4: Production-Ready Systems
ENABLE_AI_MONITORING=false           # Lesson 10: Agent performance monitoring
ENABLE_AI_SECURITY=false             # Lesson 11: Security and compliance controls
ENABLE_AI_SCALING=false              # Lesson 12: Performance optimization

# =============================================================================
# APPLICATION SETTINGS
# =============================================================================
LOG_LEVEL=INFO
ENVIRONMENT=development

# Metrics Collection (for measuring AI impact)
ENABLE_METRICS=true                  # Track performance metrics for before/after comparison
METRICS_EXPORT_INTERVAL=60           # Export metrics every N seconds

# Optional: Azure OpenAI (if you want to use Azure instead of GitHub Copilot)
# AZURE_OPENAI_ENDPOINT=
# AZURE_OPENAI_API_KEY=
# AZURE_OPENAI_DEPLOYMENT=
